# PySpark Docker API

ğŸš€ **Sistema completo de processamento de dados de tÃ¡xi NYC com PySpark e FastAPI**

## ğŸ“‹ Funcionalidades

- âœ… **Processamento distribuÃ­do** com Apache Spark
- âœ… **API REST** com FastAPI para consulta de dados
- âœ… **Suporte Docker** para fÃ¡cil deploy
- âœ… **OtimizaÃ§Ãµes avanÃ§adas** de performance
- âœ… **Filtros dinÃ¢micos** e estatÃ­sticas
- âœ… **Dados em formato Parquet** comprimido

## ğŸ—ï¸ Arquitetura
```text
[CSV Raw Data] â†’ [PySpark Job] â†’ [Parquet Files] â†’ [FastAPI] â†’ [User/API Client]

```
â”œâ”€â”€ api/                 # FastAPI application
â”‚   â”œâ”€â”€ main.py         # Endpoints REST
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ scripts/            # Scripts de processamento
â”‚   â”œâ”€â”€ spark_job.py    # Job principal Spark
â”‚   â”œâ”€â”€ split_file.sh   # DivisÃ£o de arquivos
â”‚   â””â”€â”€ consolidate_chunks.py
â”œâ”€â”€ data/               # Dados (excluÃ­do do Git)
â”‚   â”œâ”€â”€ raw/           # Dados originais CSV
â”‚   â”œâ”€â”€ split_raw/     # Arquivos divididos
â”‚   â””â”€â”€ processed/     # Dados processados Parquet
â””â”€â”€ Dockerfile.spark    # Container Spark
```

## ğŸš€ Como Usar

### 1. PreparaÃ§Ã£o dos Dados
```bash
# Dividir arquivo CSV grande em chunks menores
cd scripts
./split_file.sh
```

### 2. Processamento com Spark
```bash
# Executar job de processamento
python scripts/spark_job.py
```

### 3. API REST
```bash
# Instalar dependÃªncias
pip install -r api/requirements.txt

# Executar API
cd api
uvicorn main:app --host 0.0.0.0 --port 8001
```

### 4. Docker (Alternativo)
```bash
# Build da imagem Spark
docker build -f Dockerfile.spark -t taxi-spark-job .

# Executar processamento
docker run -v $(pwd)/data:/opt/data taxi-spark-job
```

## ğŸ“Š Endpoints da API

- `GET /` - Health check
- `GET /info` - InformaÃ§Ãµes do dataset
- `POST /filter` - Filtros dinÃ¢micos
- `POST /stats` - EstatÃ­sticas por colunas
- `GET /samples` - Amostras de dados

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)
```bash
CSV_PATH=./data/raw/2018_Yellow_Taxi_Trip_Data.csv
PARQUET_PATH=./data/processed/taxi_clean.parquet
API_PORT=8001
```

### OtimizaÃ§Ãµes Spark
- **Adaptive Query Execution** habilitado
- **Column pruning** automÃ¡tico
- **Predicate pushdown** otimizado
- **CompressÃ£o Snappy** para Parquet
- **Particionamento** inteligente

## ğŸ“ˆ Performance

### Dados Processados
- **~113 milhÃµes** de registros
- **20 arquivos** CSV divididos
- **Processamento paralelo** em 20 partiÃ§Ãµes
- **Formato Parquet** otimizado

### Features Calculadas
- DuraÃ§Ã£o da viagem (minutos)
- Velocidade mÃ©dia (mph)
- Hora do pickup
- Dia da semana
- Porcentagem de gorjeta

## ğŸ› ï¸ Tecnologias

- **Apache Spark 3.2+** - Processamento distribuÃ­do
- **Python 3.11+** - Linguagem principal
- **FastAPI** - API REST moderna
- **Pandas** - ManipulaÃ§Ã£o de dados
- **PyArrow** - Leitura Parquet otimizada
- **Docker** - ContainerizaÃ§Ã£o
- **Bitnami Spark** - Imagem Docker otimizada

## ğŸ“ LicenÃ§a

MIT License - Veja [LICENSE](LICENSE) para detalhes.

---

ğŸ’¡ **Desenvolvido para anÃ¡lise de dados de tÃ¡xi de NYC com foco em performance e escalabilidade.**