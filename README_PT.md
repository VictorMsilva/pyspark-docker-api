# PySpark Docker API

ğŸš€ **Sistema completo de processamento de dados de tÃ¡xi NYC com PySpark e FastAPI**

## ğŸ“‹ Funcionalidades

- âœ… **Processamento distribuÃ­do** com Apache Spark
- âœ… **API REST** com FastAPI para consulta de dados
- âœ… **Suporte Docker** para fÃ¡cil deploy
- âœ… **OtimizaÃ§Ãµes avanÃ§adas** de performance
- âœ… **Filtros dinÃ¢micos** e estatÃ­sticas
- âœ… **Dados em formato Parquet** comprimido

## ğŸ—ï¸ Arquitetura
```text
[CSV Raw Data] â†’ [PySpark Job] â†’ [Parquet Files] â†’ [FastAPI] â†’ [User/API Client]

```
â”œâ”€â”€ api/                 # FastAPI application
â”‚   â”œâ”€â”€ main.py         # Endpoints REST
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ scripts/            # Scripts de processamento
â”‚   â”œâ”€â”€ spark_job.py    # Job principal Spark
â”‚   â”œâ”€â”€ split_file.sh   # DivisÃ£o de arquivos
â”‚   â”œâ”€â”€ docker-control.sh # Controle Docker
â”‚   â””â”€â”€ consolidate_chunks.py
â”œâ”€â”€ data/               # Dados (exemplos incluÃ­dos)
â”‚   â”œâ”€â”€ raw/           # Dados originais CSV
â”‚   â”œâ”€â”€ split_raw/     # Arquivos divididos
â”‚   â””â”€â”€ processed/     # Dados processados Parquet
â”œâ”€â”€ Dockerfile.spark    # Container Spark
â”œâ”€â”€ Dockerfile.api      # Container API
â””â”€â”€ docker-compose.yml  # OrquestraÃ§Ã£o completa
```

## ğŸš€ Como Usar

### 1. Docker Compose (Recomendado)
```bash
# Iniciar cluster Spark + API
./scripts/docker-control.sh start

# Executar apenas o job Spark
./scripts/docker-control.sh job

# Iniciar apenas a API
./scripts/docker-control.sh api

# Parar todos os serviÃ§os
./scripts/docker-control.sh stop

# Ver logs
./scripts/docker-control.sh logs

# Limpeza completa
./scripts/docker-control.sh clean
```

### 2. ExecuÃ§Ã£o Local

#### PreparaÃ§Ã£o dos Dados
```bash
# Dividir arquivo CSV grande em chunks menores
cd scripts
./split_file.sh
```

#### Processamento com Spark
```bash
# Executar job de processamento
python scripts/spark_job.py
```

#### API REST
```bash
# Instalar dependÃªncias
pip install -r api/requirements.txt

# Executar API
cd api
uvicorn main:app --host 0.0.0.0 --port 8001
```

### 3. Docker Manual
```bash
# Build das imagens
docker build -f Dockerfile.spark -t taxi-spark-job .
docker build -f Dockerfile.api -t taxi-api .

# Executar containers
docker run -v $(pwd)/data:/opt/data taxi-spark-job
docker run -p 8001:8001 -v $(pwd)/data:/app/data taxi-api
```

## ğŸ“Š Endpoints da API

ApÃ³s iniciar os containers, a API estarÃ¡ disponÃ­vel em `http://localhost:8001`

- `GET /` - Health check
- `GET /info` - InformaÃ§Ãµes do dataset
- `POST /filter` - Filtros dinÃ¢micos
- `POST /stats` - EstatÃ­sticas por colunas
- `GET /samples` - Amostras de dados

### Interfaces Web
- **API Documentation**: http://localhost:8001/docs
- **Spark Master UI**: http://localhost:8080
- **Spark Worker UI**: http://localhost:8081

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente (.env)
```bash
# Dados
CSV_PATH=./data/raw/2018_Yellow_Taxi_Trip_Data.csv
PARQUET_PATH=./data/processed/taxi_clean.parquet

# API
API_PORT=8001

# Spark (para Docker)
SPARK_MASTER_URL=spark://spark-master:7077
SPARK_WORKER_MEMORY=4g
SPARK_WORKER_CORES=2
```

### OtimizaÃ§Ãµes Spark
- **Adaptive Query Execution** habilitado
- **Column pruning** automÃ¡tico
- **Predicate pushdown** otimizado
- **CompressÃ£o Snappy** para Parquet
- **Particionamento** inteligente

## ğŸ“ˆ Performance

### Dados Processados
- **~113 milhÃµes** de registros
- **20 arquivos** CSV divididos
- **Processamento paralelo** em 20 partiÃ§Ãµes
- **Formato Parquet** otimizado

### Features Calculadas
- DuraÃ§Ã£o da viagem (minutos)
- Velocidade mÃ©dia (mph)
- Hora do pickup
- Dia da semana
- Porcentagem de gorjeta

## ğŸ› ï¸ Tecnologias

- **Apache Spark 3.5+** - Processamento distribuÃ­do
- **Python 3.11+** - Linguagem principal
- **FastAPI** - API REST moderna
- **Pandas** - ManipulaÃ§Ã£o de dados
- **PyArrow** - Leitura Parquet otimizada
- **Docker & Docker Compose** - ContainerizaÃ§Ã£o
- **Bitnami Spark** - Imagem Docker otimizada

## ğŸ³ Containers

### Spark Cluster
- **spark-master**: Coordenador do cluster (porta 8080)
- **spark-worker**: Worker node (porta 8081)
- **spark-job**: Executor de jobs (profile: job)

### API
- **taxi-api**: FastAPI service (porta 8001)

### Volumes
- `./data`: Dados compartilhados entre containers
- `./scripts`: Scripts de processamento

## ğŸ® Script de Controle

O arquivo `scripts/docker-control.sh` fornece comandos simples para gerenciar o ambiente:

```bash
# Ver todas as opÃ§Ãµes disponÃ­veis
./scripts/docker-control.sh help

# CenÃ¡rios de uso comum
./scripts/docker-control.sh start  # Ambiente completo
./scripts/docker-control.sh job    # Apenas processamento
./scripts/docker-control.sh api    # Apenas API
./scripts/docker-control.sh logs   # Ver logs em tempo real
./scripts/docker-control.sh clean  # Limpeza completa
```

## ğŸ“ LicenÃ§a

MIT License - Veja [LICENSE](LICENSE) para detalhes.

---

ğŸ’¡ **Desenvolvido para anÃ¡lise de dados de tÃ¡xi de NYC com foco em performance e escalabilidade.**